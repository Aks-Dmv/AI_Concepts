{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making NN with libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to focus on first principles, instead of trying to work with hacks and tricks. I initially used these hacks and tricks extensively to make NN. I regret this because I feel that I should have tried to understand how to code in a hard core manner.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one thing I would like to share, I feel that TensorFlow and Theano are really strong contenders if you wish to spend the time in actually making all the functions. An example of what I mean can be seen in the ANN class shown below. All that code is just crazy if you just want to prototype and use the already present stuff.  \n",
    "### Use Keras and then if you really want extra customization, write TF or theano code to do what you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano\n",
    "The grand-daddy of libraries\n",
    "#### Note: import the theano tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "A_val = np.array([[1,2], [3,4]])\n",
    "v_val = np.array([5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are three types of variables \n",
    "## These are not parameters (parameters are called shared variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just some different types of variables\n",
    "c = T.scalar('c')\n",
    "v = T.vector('v')\n",
    "A = T.matrix('A')\n",
    "\n",
    "\n",
    "# we can define a matrix multiplication\n",
    "w = A.dot(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17.  39.]\n"
     ]
    }
   ],
   "source": [
    "matrix_times_vector = theano.function(inputs=[A, v], outputs=w)\n",
    "w_val = matrix_times_vector(A_val, v_val)\n",
    "print(w_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make parameters ie. shared variables, we use the shared function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421.0\n",
      "67.99000000000001\n",
      "11.508400000000002\n",
      "2.4713440000000007\n",
      "1.0254150400000002\n",
      "0.7940664064\n",
      "0.7570506250240001\n",
      "0.75112810000384\n",
      "0.7501804960006143\n",
      "0.7500288793600982\n",
      "0.7500046206976159\n",
      "0.7500007393116186\n",
      "0.750000118289859\n",
      "0.7500000189263775\n",
      "0.7500000030282203\n",
      "0.7500000004845152\n",
      "0.7500000000775223\n",
      "0.7500000000124035\n",
      "0.7500000000019845\n",
      "0.7500000000003176\n",
      "0.7500000000000506\n",
      "0.7500000000000082\n",
      "0.7500000000000013\n",
      "0.7500000000000001\n",
      "0.7500000000000001\n",
      "-0.4999999976919052\n"
     ]
    }
   ],
   "source": [
    "x = theano.shared(20.0, 'x')\n",
    "\n",
    "# the first argument is its initial value, the second is its name\n",
    "\n",
    "# a cost function that has a minimum value\n",
    "cost = x*x + x + 1\n",
    "\n",
    "# in theano, you don't have to compute gradients yourself!\n",
    "x_update = x - 0.3*T.grad(cost, x)\n",
    "\n",
    "# x is not an \"input\", it's a thing you update\n",
    "# in later examples, data and labels would go into the inputs\n",
    "# and model params would go in the updates\n",
    "# updates takes in a list of tuples, each tuple has 2 things in it:\n",
    "# 1) the shared variable to update, 2) the update expression\n",
    "train = theano.function(inputs=[], outputs=cost, updates=[(x, x_update)])\n",
    "\n",
    "# write your own loop to call the training function.\n",
    "# it has no arguments!\n",
    "for i in range(25):\n",
    "    cost_val = train()\n",
    "    print(cost_val)\n",
    "\n",
    "# print the optimal value of x\n",
    "print(x.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with theano\n",
    "The below code doesn't include the y2indicator and get_normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with matrices here. We init the matrices for each layer using the shared function.  \n",
    "Next, we make our relu (which we put in between layers) and the softmax layer (at the end)  \n",
    "Then, we define our cost, pred, and updates.\n",
    "Finally, we make our two functions, get pred and the train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rate(p, t):\n",
    "    return np.mean(p != t)\n",
    "\n",
    "\n",
    "def relu(a):\n",
    "    return a * (a > 0)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # step 1: get the data and define all the usual variables\n",
    "    X, Y = get_normalized_data()\n",
    "\n",
    "    max_iter = 20\n",
    "    print_period = 10\n",
    "\n",
    "    lr = 0.00004\n",
    "    reg = 0.01\n",
    "\n",
    "    Xtrain = X[:-1000,]\n",
    "    Ytrain = Y[:-1000]\n",
    "    Xtest  = X[-1000:,]\n",
    "    Ytest  = Y[-1000:]\n",
    "    Ytrain_ind = y2indicator(Ytrain)\n",
    "    Ytest_ind = y2indicator(Ytest)\n",
    "\n",
    "    N, D = Xtrain.shape\n",
    "    batch_sz = 500\n",
    "    n_batches = N // batch_sz\n",
    "\n",
    "    M = 300\n",
    "    K = 10\n",
    "    W1_init = np.random.randn(D, M) / 28\n",
    "    b1_init = np.zeros(M)\n",
    "    W2_init = np.random.randn(M, K) / np.sqrt(M)\n",
    "    b2_init = np.zeros(K)\n",
    "\n",
    "    # step 2: define theano variables and expressions\n",
    "    thX = T.matrix('X')\n",
    "    thT = T.matrix('T') # Targets ie. the y actual\n",
    "    W1 = theano.shared(W1_init, 'W1')\n",
    "    b1 = theano.shared(b1_init, 'b1')\n",
    "    W2 = theano.shared(W2_init, 'W2')\n",
    "    b2 = theano.shared(b2_init, 'b2')\n",
    "\n",
    "    # we can use the built-in theano functions to do relu and softmax\n",
    "    thZ = relu( thX.dot(W1) + b1 ) # relu is new in version 0.7.1 but just in case you don't have it\n",
    "    thY = T.nnet.softmax( thZ.dot(W2) + b2 )\n",
    "\n",
    "    # define the cost function and prediction\n",
    "    cost = -(thT * T.log(thY)).sum() + reg*((W1*W1).sum() + (b1*b1).sum() + (W2*W2).sum() + (b2*b2).sum())\n",
    "    prediction = T.argmax(thY, axis=1)\n",
    "\n",
    "    # step 3: training expressions and functions\n",
    "    # we can just include regularization as part of the cost because it is also automatically differentiated!\n",
    "    # update_W1 = W1 - lr*(T.grad(cost, W1) + reg*W1)\n",
    "    # update_b1 = b1 - lr*(T.grad(cost, b1) + reg*b1)\n",
    "    # update_W2 = W2 - lr*(T.grad(cost, W2) + reg*W2)\n",
    "    # update_b2 = b2 - lr*(T.grad(cost, b2) + reg*b2)\n",
    "    update_W1 = W1 - lr*T.grad(cost, W1)\n",
    "    update_b1 = b1 - lr*T.grad(cost, b1)\n",
    "    update_W2 = W2 - lr*T.grad(cost, W2)\n",
    "    update_b2 = b2 - lr*T.grad(cost, b2)\n",
    "\n",
    "    train = theano.function(\n",
    "        inputs=[thX, thT],\n",
    "        updates=[(W1, update_W1), (b1, update_b1), (W2, update_W2), (b2, update_b2)],\n",
    "    )\n",
    "\n",
    "    # create another function for this because we want it over the whole dataset\n",
    "    get_prediction = theano.function(\n",
    "        inputs=[thX, thT],\n",
    "        outputs=[cost, prediction],\n",
    "    )\n",
    "\n",
    "    costs = []\n",
    "    for i in range(max_iter):\n",
    "        for j in range(n_batches):\n",
    "            Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "            Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "\n",
    "            train(Xbatch, Ybatch)\n",
    "            if j % print_period == 0:\n",
    "                cost_val, prediction_val = get_prediction(Xtest, Ytest_ind)\n",
    "                err = error_rate(prediction_val, Ytest)\n",
    "                print(\"Cost / err at iteration i=%d, j=%d: %.3f / %.3f\" % (i, j, cost_val, err))\n",
    "                costs.append(cost_val)\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN class in theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(M1, M2):\n",
    "  return np.random.randn(M1, M2) * np.sqrt(2.0 / M1)\n",
    "\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, M1, M2, f):\n",
    "        self.M1 = M1\n",
    "        self.M2 = M2\n",
    "        self.f = f\n",
    "        W = init_weight(M1, M2)\n",
    "        b = np.zeros(M2)\n",
    "        self.W = theano.shared(W)\n",
    "        self.b = theano.shared(b)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.f == T.nnet.relu:\n",
    "            return self.f(X.dot(self.W) + self.b, alpha=0.1)\n",
    "        return self.f(X.dot(self.W) + self.b)\n",
    "\n",
    "\n",
    "class ANN(object):\n",
    "    def __init__(self, hidden_layer_sizes):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "    def fit(self, X, Y, activation=T.nnet.relu, learning_rate=1e-3, mu=0.0, reg=0, epochs=100, batch_sz=None, print_period=100, show_fig=True):\n",
    "        X = X.astype(np.float32)\n",
    "        Y = Y.astype(np.int32)\n",
    "\n",
    "        # initialize hidden layers\n",
    "        N, D = X.shape\n",
    "        self.layers = []\n",
    "        M1 = D\n",
    "        for M2 in self.hidden_layer_sizes:\n",
    "            h = HiddenLayer(M1, M2, activation)\n",
    "            self.layers.append(h)\n",
    "            M1 = M2\n",
    "        \n",
    "        # final layer\n",
    "        K = len(set(Y))\n",
    "        # print(\"K:\", K)\n",
    "        h = HiddenLayer(M1, K, T.nnet.softmax)\n",
    "        self.layers.append(h)\n",
    "\n",
    "        if batch_sz is None:\n",
    "            batch_sz = N\n",
    "\n",
    "        # collect params for later use\n",
    "        self.params = []\n",
    "        for h in self.layers:\n",
    "            self.params += h.params\n",
    "\n",
    "        # for momentum\n",
    "        dparams = [theano.shared(np.zeros_like(p.get_value())) for p in self.params]\n",
    "\n",
    "        # set up theano functions and variables\n",
    "        thX = T.matrix('X')\n",
    "        thY = T.ivector('Y')\n",
    "        p_y_given_x = self.forward(thX)\n",
    "\n",
    "        rcost = reg*T.mean([(p*p).sum() for p in self.params])\n",
    "        cost = -T.mean(T.log(p_y_given_x[T.arange(thY.shape[0]), thY])) #+ rcost\n",
    "        prediction = T.argmax(p_y_given_x, axis=1)\n",
    "        grads = T.grad(cost, self.params)\n",
    "\n",
    "        # momentum only\n",
    "        updates = [\n",
    "            (p, p + mu*dp - learning_rate*g) for p, dp, g in zip(self.params, dparams, grads)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate*g) for dp, g in zip(dparams, grads)\n",
    "        ]\n",
    "\n",
    "        train_op = theano.function(\n",
    "            inputs=[thX, thY],\n",
    "            outputs=[cost, prediction],\n",
    "            updates=updates,\n",
    "        )\n",
    "\n",
    "        self.predict_op = theano.function(\n",
    "            inputs=[thX],\n",
    "            outputs=prediction,\n",
    "        )\n",
    "\n",
    "        n_batches = N // batch_sz\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            if n_batches > 1:\n",
    "              X, Y = shuffle(X, Y)\n",
    "            for j in range(n_batches):\n",
    "                Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "                Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "\n",
    "                c, p = train_op(Xbatch, Ybatch)\n",
    "                costs.append(c)\n",
    "                if (j+1) % print_period == 0:\n",
    "                    print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "        for h in self.layers:\n",
    "            out = h.forward(out)\n",
    "        return out\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        P = self.predict_op(X)\n",
    "        return np.mean(Y == P)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_op(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, some similarities between the two  \n",
    "Instead of vectors, matrices, and scalars(ie. the variables in theano), in their place we have this placeholder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(5, 5), name='A')\n",
    "\n",
    "\n",
    "# but shape and name are optional\n",
    "v = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# I think this name is more appropriate than 'dot'\n",
    "w = tf.matmul(A, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to begin computation, we have to make this session construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94206238]\n",
      " [ 3.06947494]\n",
      " [-0.14446139]\n",
      " [-0.83221436]\n",
      " [-0.27277359]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    # the values are fed in via the appropriately named argument \"feed_dict\"\n",
    "    # v needs to be of shape=(5, 1) not just shape=(5,)\n",
    "    # it's more like \"real\" matrix multiplication\n",
    "    output = session.run(w, feed_dict={A: np.random.randn(5, 5), v: np.random.randn(5, 1)})\n",
    "\n",
    "    # what's this output that is returned by the session? let's print it\n",
    "    print(output, type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow variables are theano shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[-1.93149292 -0.47283894]\n",
      " [ 0.76835477  0.14378241]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# A tf variable can be initialized with a numpy array or a tf array\n",
    "# or more correctly, anything that can be turned into a tf tensor\n",
    "shape = (2, 2)\n",
    "x = tf.Variable(tf.random_normal(shape))\n",
    "# x = tf.Variable(np.random.randn(2, 2))\n",
    "t = tf.Variable(0) # a scalar\n",
    "\n",
    "# you need to \"initialize\" the variables first\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    out = session.run(init) # and then \"run\" the init operation\n",
    "    print(out) # it's just None\n",
    "\n",
    "    # eval() in tf is like get_value() in Theano\n",
    "    print(x.eval()) # the initial value of x\n",
    "    print(t.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do our update equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0, cost = 67.990, u = 7.700\n",
      "i = 1, cost = 11.508, u = 2.780\n",
      "i = 2, cost = 2.471, u = 0.812\n",
      "i = 3, cost = 1.025, u = 0.025\n",
      "i = 4, cost = 0.794, u = -0.290\n",
      "i = 5, cost = 0.757, u = -0.416\n",
      "i = 6, cost = 0.751, u = -0.466\n",
      "i = 7, cost = 0.750, u = -0.487\n",
      "i = 8, cost = 0.750, u = -0.495\n",
      "i = 9, cost = 0.750, u = -0.498\n",
      "i = 10, cost = 0.750, u = -0.499\n",
      "i = 11, cost = 0.750, u = -0.500\n"
     ]
    }
   ],
   "source": [
    "# let's now try to find the minimum of a simple cost function like we did in Theano\n",
    "u = tf.Variable(20.0)\n",
    "cost = u*u + u + 1.0\n",
    "\n",
    "# One difference between Theano and TensorFlow is that you don't write the updates\n",
    "# yourself in TensorFlow. You choose an optimizer that implements the algorithm you want.\n",
    "# 0.3 is the learning rate. Documentation lists the params.\n",
    "train_op = tf.train.GradientDescentOptimizer(0.3).minimize(cost)\n",
    "\n",
    "# let's run a session again\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    # Strangely, while the weight update is automated, the loop itself is not.\n",
    "    # So we'll just call train_op until convergence.\n",
    "    # This is useful for us anyway since we want to track the cost function.\n",
    "    for i in range(12):\n",
    "        session.run(train_op)\n",
    "        print(\"i = %d, cost = %.3f, u = %.3f\" % (i, cost.eval(), u.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN in TF\n",
    "It is really similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rate(p, t):\n",
    "    return np.mean(p != t)\n",
    "\n",
    "\n",
    "# copy this first part from theano2.py\n",
    "def main():\n",
    "    # step 1: get the data and define all the usual variables\n",
    "    X, Y = get_normalized_data()\n",
    "\n",
    "    max_iter = 15\n",
    "    print_period = 10\n",
    "\n",
    "    lr = 0.00004\n",
    "    reg = 0.01\n",
    "\n",
    "    Xtrain = X[:-1000,]\n",
    "    Ytrain = Y[:-1000]\n",
    "    Xtest  = X[-1000:,]\n",
    "    Ytest  = Y[-1000:]\n",
    "    Ytrain_ind = y2indicator(Ytrain)\n",
    "    Ytest_ind = y2indicator(Ytest)\n",
    "\n",
    "    N, D = Xtrain.shape\n",
    "    batch_sz = 500\n",
    "    n_batches = N // batch_sz\n",
    "\n",
    "    # add an extra layer just for fun\n",
    "    M1 = 300\n",
    "    M2 = 100\n",
    "    K = 10\n",
    "    W1_init = np.random.randn(D, M1) / 28\n",
    "    b1_init = np.zeros(M1)\n",
    "    W2_init = np.random.randn(M1, M2) / np.sqrt(M1)\n",
    "    b2_init = np.zeros(M2)\n",
    "    W3_init = np.random.randn(M2, K) / np.sqrt(M2)\n",
    "    b3_init = np.zeros(K)\n",
    "\n",
    "\n",
    "    # define variables and expressions\n",
    "    X = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "    T = tf.placeholder(tf.float32, shape=(None, K), name='T')\n",
    "    W1 = tf.Variable(W1_init.astype(np.float32))\n",
    "    b1 = tf.Variable(b1_init.astype(np.float32))\n",
    "    W2 = tf.Variable(W2_init.astype(np.float32))\n",
    "    b2 = tf.Variable(b2_init.astype(np.float32))\n",
    "    W3 = tf.Variable(W3_init.astype(np.float32))\n",
    "    b3 = tf.Variable(b3_init.astype(np.float32))\n",
    "\n",
    "    # define the model\n",
    "    Z1 = tf.nn.relu( tf.matmul(X, W1) + b1 )\n",
    "    Z2 = tf.nn.relu( tf.matmul(Z1, W2) + b2 )\n",
    "    Yish = tf.matmul(Z2, W3) + b3 # remember, the cost function does the softmaxing by default in TF\n",
    "\n",
    "    # softmax_cross_entropy_with_logits take in the \"logits\"\n",
    "    # if you wanted to know the actual output of the neural net,\n",
    "    # you could pass \"Yish\" into tf.nn.softmax(logits)\n",
    "    cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=Yish, labels=T))\n",
    "\n",
    "    # we choose the optimizer but don't implement the algorithm ourselves\n",
    "    # let's go with RMSprop, since we just learned about it.\n",
    "    # it includes momentum!\n",
    "    train_op = tf.train.RMSPropOptimizer(lr, decay=0.99, momentum=0.9).minimize(cost)\n",
    "\n",
    "    # we'll use this to calculate the error rate\n",
    "    predict_op = tf.argmax(Yish, 1)\n",
    "\n",
    "    costs = []\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            for j in range(n_batches):\n",
    "                Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "                Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "\n",
    "                session.run(train_op, feed_dict={X: Xbatch, T: Ybatch})\n",
    "                if j % print_period == 0: #Just for printing\n",
    "                    test_cost = session.run(cost, feed_dict={X: Xtest, T: Ytest_ind})\n",
    "                    prediction = session.run(predict_op, feed_dict={X: Xtest})\n",
    "                    err = error_rate(prediction, Ytest)\n",
    "                    print(\"Cost / err at iteration i=%d, j=%d: %.3f / %.3f\" % (i, j, test_cost, err))\n",
    "                    costs.append(test_cost)\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "The only difference in the code will occur in the forward prop during train and test functions, although the differences in both are quite distinct.  \n",
    "The train would mask (theano) the nodes (TF has a function that does this for you) and the test would multiply each output with p_keep in order to get an ensemble effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano code\n",
    "Only change would be in the forward prop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_train(self, X):\n",
    "    Z = X\n",
    "    for h, p in zip(self.hidden_layers, self.dropout_rates[:-1]):\n",
    "        mask = self.rng.binomial(n=1, p=p, size=Z.shape)\n",
    "        Z = mask * Z\n",
    "        Z = h.forward(Z)\n",
    "    mask = self.rng.binomial(n=1, p=self.dropout_rates[-1], size=Z.shape)\n",
    "    Z = mask * Z\n",
    "    return T.nnet.softmax(Z.dot(self.W) + self.b)\n",
    "\n",
    "def forward_predict(self, X):\n",
    "    Z = X\n",
    "    for h, p in zip(self.hidden_layers, self.dropout_rates[:-1]):\n",
    "        Z = h.forward(p * Z)\n",
    "    return T.nnet.softmax((self.dropout_rates[-1] * Z).dot(self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow code\n",
    "again change would occur in the forward prop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(self, X):\n",
    "    # tf.nn.dropout scales inputs by 1/p_keep\n",
    "    # therefore, during test time, we don't have to scale anything\n",
    "    Z = X\n",
    "    Z = tf.nn.dropout(Z, self.dropout_rates[0])\n",
    "    # The above is for the inputs\n",
    "    # The below is for the hidden layer\n",
    "    for h, p in zip(self.hidden_layers, self.dropout_rates[1:]):\n",
    "        Z = h.forward(Z)\n",
    "        Z = tf.nn.dropout(Z, p)\n",
    "    return tf.matmul(Z, self.W) + self.b\n",
    "\n",
    "def forward_test(self, X):\n",
    "    Z = X\n",
    "    for h in self.hidden_layers:\n",
    "        Z = h.forward(Z)\n",
    "    return tf.matmul(Z, self.W) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm\n",
    "only will change in the Hidden Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayerBatchNormTheano(object):\n",
    "  def __init__(self, M1, M2, f):\n",
    "    self.M1 = M1\n",
    "    self.M2 = M2\n",
    "    self.f = f\n",
    "\n",
    "    W = init_weight(M1, M2)\n",
    "    gamma = np.ones(M2)\n",
    "    beta = np.zeros(M2)\n",
    "\n",
    "    self.W = theano.shared(W)\n",
    "    self.gamma = theano.shared(gamma)\n",
    "    self.beta = theano.shared(beta)\n",
    "\n",
    "    self.params = [self.W, self.gamma, self.beta]\n",
    "\n",
    "    # for test time\n",
    "    # self.running_mean = T.zeros(M2)\n",
    "    # self.running_var = T.zeros(M2)\n",
    "    self.running_mean = theano.shared(np.zeros(M2))\n",
    "    self.running_var = theano.shared(np.zeros(M2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayerBatchNormTF(object):\n",
    "  def __init__(self, M1, M2, f):\n",
    "    self.M1 = M1\n",
    "    self.M2 = M2\n",
    "    self.f = f\n",
    "\n",
    "    W = init_weight(M1, M2).astype(np.float32)\n",
    "    gamma = np.ones(M2).astype(np.float32)\n",
    "    beta = np.zeros(M2).astype(np.float32)\n",
    "\n",
    "    self.W = tf.Variable(W)\n",
    "    self.gamma = tf.Variable(gamma)\n",
    "    self.beta = tf.Variable(beta)\n",
    "\n",
    "    # for test time\n",
    "    self.running_mean = tf.Variable(np.zeros(M2).astype(np.float32), trainable=False)\n",
    "    self.running_var = tf.Variable(np.zeros(M2).astype(np.float32), trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
