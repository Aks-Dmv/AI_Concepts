{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we can basically say that this notebook will cover the basic MDP algo's like policy and value iteration along with the use of NN for Q Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will just be coding up the basic algorithms and explaining them.  \n",
    "1) What is a value function?\n",
    "Ans. It is a black box that gives the \"Value\" of a state (input).  \n",
    "The function below is mainly used to get your value function (to be returned).  \n",
    "  \n",
    "The algo is simple:\n",
    "1) We run a loop till the max update to your Value function is less than theta. Cool, that handles the while, theta and the delta stuff.  \n",
    "2) We run a for loop on all the states. Cool, the for loop is done.  \n",
    "3) the double for loop is just  \n",
    "  \n",
    "**V(s) = E<sub>A</sub> [ P (A | s) \\* $\\Sigma$<sub>S'</sub> ( P (S' | A, s) \\* [ r(s) + gamma * V(S') ] ) ]** \n",
    "  \n",
    "Ok, let's explain this. First off, the P is stochastic, meaning that we get a prob of it being A. **Note: this is not $\\Pi$ where we get an action as output.** However, pick one and keep it in mind. The A and the S' are random variables, thus the capitals, while the s is a value, hence the lowercase. Now, for every A, we calculate the prob (as shown) multiplied with the sum of the prob of going to state S'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 1.0\n",
    "THETA = 1e-5\n",
    "\n",
    "def policy_evaluation(policy, env):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    transitions = env.P\n",
    "    # initialize an array V(s) = 0, for all s in S+\n",
    "    V = np.array(np.zeros(num_states))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(num_states):\n",
    "            # v = V(s)\n",
    "            new_value = 0\n",
    "            for action, p_action in enumerate(policy[state]):\n",
    "                # sum over s', r\n",
    "                for probability, nextstate, reward, _ in transitions[state][action]:\n",
    "                    new_value += p_action * probability * (reward + GAMMA * V[nextstate])\n",
    "            # delta = max(delta, abs(v - V(s)))\n",
    "            delta = max(delta, np.abs(new_value - V[state]))\n",
    "            V[state] = new_value\n",
    "        # until delta < theta\n",
    "        if delta < THETA:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now that we finished how to get the value function after having the policy, we move on to policy iteration. In this function, we give an environment and a policy, we let it explore (iterate) and then it give us back the optimal policy.  \n",
    "Update rule is: $\\Pi$ (s) = argmax<sub>A</sub>(sum( P (s' | s, a ) * [ r(s) + gamma * V (s') ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(policy, env):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    transitions = env.P\n",
    "\n",
    "    policy_stable = False\n",
    "\n",
    "    while not(policy_stable):\n",
    "        # step 2: policy evaluation\n",
    "        V = policy_evaluation(policy, env)\n",
    "        # for each s in S:\n",
    "        for state in range(num_states):\n",
    "            # old_action = pi_s\n",
    "            old_action = np.argmax(policy[state])\n",
    "            \n",
    "            new_action_values = np.zeros(num_actions)\n",
    "            for action in range(num_actions):\n",
    "                for probability, nextstate, reward, _ in transitions[state][action]:\n",
    "                    new_action_values[action] += probability * (reward + GAMMA * V[nextstate])\n",
    "            new_action = np.argmax(new_action_values)\n",
    "            # update policy: policy[state] is pi_s\n",
    "            # if policy_stable, then stop and return optimal policies and value, else go to step 2\n",
    "            policy_stable = (new_action == old_action)\n",
    "            policy[state] = np.eye(num_actions)[new_action]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the implementation may seem very naive, it works in practice. Ok, how about moving to Value Iteration?  \n",
    "  \n",
    "Here, we return a policy and a value function. This should be simple to understand.  \n",
    "\n",
    "The first double for loop is the value iteration update and the next double for loop is just finding **argmax**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    transitions = env.P\n",
    "    # initialize array V arbitarily\n",
    "    V = np.zeros(num_states)\n",
    "    # repeat\n",
    "    while True:\n",
    "        # initialize delta to zero\n",
    "        delta = 0\n",
    "        # for each s in S\n",
    "        for state in range(num_states):\n",
    "            # v = V(s)\n",
    "            old_value = V[state]\n",
    "            new_action_values = np.zeros(num_actions)\n",
    "            # update rule is: V(s) = max_a(sum(p(s',r|s,a) * [r + gamma * V(s')]))\n",
    "            for action in range(num_actions):\n",
    "                for probability, nextstate, reward, _ in transitions[state][action]:\n",
    "                    new_action_values[action] += probability * (reward + GAMMA * V[nextstate])\n",
    "            V[state] = np.max(new_action_values)\n",
    "            # calculate delta = max(delta, |v - V(s)|)\n",
    "            delta = max(delta, np.abs(V[state] - old_value))\n",
    "        # until delta is smaller than theta\n",
    "        if delta < THETA:\n",
    "            break\n",
    "\n",
    "    # output a deterministic policy (the optimal one)\n",
    "    optimal_policy = np.zeros([num_states, num_actions])\n",
    "    for state in range(num_states):\n",
    "        action_values = np.zeros(num_actions)\n",
    "        for action in range(num_actions):\n",
    "            for probability, nextstate, reward, _ in transitions[state][action]:\n",
    "                action_values[action] += probability * (reward + GAMMA * V[nextstate])\n",
    "        optimal_policy[state] = np.eye(num_actions)[np.argmax(action_values)]\n",
    "\n",
    "    return optimal_policy, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
