{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is Akshay's Beginning RL Tutorial  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory isn't covered here, just code and explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To download gym, make a .sh file and put the following:  \n",
    "git clone https://github.com/openai/gym  \n",
    "cd gym  \n",
    "pip install -e . # minimal install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is just to print all the different environments on OpenAI gym  \n",
    "Cool, so now we can pick whatever we want  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for envs in gym.envs.registry.all():\n",
    "#    print(envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to make our environment, Cart Pole v0 and reset it.\n",
    "  \n",
    "If you get an **error**, like  \n",
    "  \n",
    "**\"gym.spaces.Box autodetected dtype as . Please provide explicit dtype\"**  \n",
    "  \n",
    "then just cd gym/spaces and in box.py change dtype of init to np.float32  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation=env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell just prints the number of possible actions and one such action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(env.action_space)\n",
    "#print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't \"RENDER\" in Jupyter Notebooks\n",
    "### Just make a python file and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    #env.render()\n",
    "    # Render has been commented out\n",
    "    print(observation)\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time.sleep(0.1)\n",
    "    if done:\n",
    "        env.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# OK, We finished the setup phase\n",
    "## Now let us move on to making classes to control the RL\n",
    "  \n",
    "The basic procedure is the same  \n",
    "  \n",
    "**1) First make a Harness class to use once you made your agent.  \n",
    "2) Next make an Agent class which has parameters and a policy function. (parameters are for your policy)  \n",
    "3) Finally make an external or internal method to train.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Harness class is used to harness the power of our agent and its environment.  \n",
    "It basically runs the episodes when an agent and a env is passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Harness:\n",
    "\n",
    "    def run_episode(self, env, agent):\n",
    "        observation = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(1000):\n",
    "            action = agent.next_action(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Linear Agent has a linear function approximation of the Policy\n",
    "OK, OK, what do we mean by that?  \n",
    "Well, a policy is something that gives us the action we need to take when we give it our **state**.  \n",
    "  \n",
    "\n",
    "$\\Pi$\\*(s) = argmax<sub>a</sub> E[Q(s,a)] = the best action  \n",
    "  \n",
    "The linear agent takes in vector of size 4 and dot products it with its own vector of size 4.  \n",
    "## Note: Because this is a continuous space problem, we didn't use discrete DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        # We use *2-1 because we know that numbers around 1 will do the trick\n",
    "        self.parameters = np.random.rand(4) * 2 - 1\n",
    "\n",
    "    def next_action(self, observation):\n",
    "        return 0 if np.matmul(self.parameters, observation) < 0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For any training function\n",
    "Steps  \n",
    "**\n",
    "1) Make the env  \n",
    "2) init params, rewards, agent, harness  \n",
    "3) for loop of harness running agent, seeing rewards, updating param**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    best_params = None\n",
    "    best_reward = 0\n",
    "    agent = LinearAgent()\n",
    "    harness = Harness()\n",
    "\n",
    "    for step in range(10000):\n",
    "        agent.parameters = np.random.rand(4) * 2 - 1\n",
    "        reward = harness.run_episode(env, agent)\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_params = agent.parameters\n",
    "            if reward == 200:\n",
    "                print('200 achieved on step {}'.format(step))\n",
    "\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hill_climbing():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    noise_scaling = 0.1\n",
    "    best_reward = 0\n",
    "    reward = 0\n",
    "    agent = LinearAgent()\n",
    "    harness = Harness()\n",
    "\n",
    "    for step in range(10000):\n",
    "        old_params = agent.parameters\n",
    "        agent.parameters += noise_scaling * (np.random.rand(4) * 2 - 1)\n",
    "        run = harness.run_episode(env, agent)\n",
    "        if run > best_reward:\n",
    "            best_reward = run\n",
    "            print('Step: {}, New Record: {}, Policy: {}'.format(step, best_reward, agent.parameters))\n",
    "        else:\n",
    "            agent.parameters = old_params\n",
    "\n",
    "        if reward == 200:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let us make an MAB environment\n",
    "Here we will not use the gym library.  \n",
    "  \n",
    "**This is to show how we don't need gym to do work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bandit = [0.2, 0.0, 0.1, -4.0]\n",
    "        self.num_actions = 4\n",
    "\n",
    "    def pull(self, arm):\n",
    "        return 1 if np.random.randn(1) > self.bandit[arm] else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an excellent way to introduce probabilities. Basically, to get a certain prob we just have to pick an x0 where P[X>x0]=the value we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, now we will introduce tensorflow into our agent's policy function\n",
    "  \n",
    "How are we using tensorflow here?  \n",
    "We start of by keeping place holders for reward at time t-1, action we did at t-1, and the weight matrix. **We are not multiplying the weight matrix in this example.** Here, we will just keep a **single number** to that will be modified by the loss function. We obtain this single number by the split function.  \n",
    "  \n",
    "Instead of doing that slice stuff, you can just use one hot encoding and matmul as shown in contextual bandit.\n",
    "  \n",
    "The predict function just runs best action. The rand or pred is $\\epsilon$ greedy. The train function runs the optimiser.  \n",
    "This is like 4 different NN, where each activates only when we want them to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, actions=4):\n",
    "        self.num_actions = actions\n",
    "        self.reward_in = tf.placeholder(tf.float32, [1], name='reward_in')\n",
    "        self.action_in = tf.placeholder(tf.int32, [1], name='action_in')\n",
    "\n",
    "        self.W = tf.get_variable('W', [self.num_actions])\n",
    "        self.best_action = tf.argmax(self.W, axis=0)\n",
    "\n",
    "        action_weight = tf.slice(self.W, self.action_in, [1])\n",
    "        policy_loss = -(tf.log(action_weight) * self.reward_in)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(policy_loss)\n",
    "\n",
    "    def predict(self, sess):\n",
    "        return sess.run(self.best_action)\n",
    "\n",
    "    def random_or_predict(self, sess, epsilon):\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return self.predict(sess)\n",
    "\n",
    "    def train(self, sess, action, reward):\n",
    "        #The optimiser will calculate the gradient and do one update.\n",
    "        #This is like mini batch training\n",
    "        sess.run(self.optimizer, {\n",
    "            self.action_in: [action],\n",
    "            self.reward_in: [reward]\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = MultiArmedBandit()\n",
    "agent = Agent()\n",
    "EPSILON = 0.1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # This is init function of TF\n",
    "    for _ in range(100000):\n",
    "        action = agent.random_or_predict(sess, EPSILON)\n",
    "        reward = env.pull(action)\n",
    "        agent.train(sess, action, reward)\n",
    "    \n",
    "    # results time\n",
    "    print(np.argmin(np.array(env.bandit)))\n",
    "    #The one below will just see how well our prediction is\n",
    "    #If it matches above then cool\n",
    "    print(agent.predict(sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I observed that even if we run the code for 50,000 iterations, it doesn't converge. We get outputs like 3 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for Contextual Bandit\n",
    "This now has states and a get bandit function. We need this function because our RL agent needs to sense its environment. It needs to know which state it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContextualBandit:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.active_bandit = 0  # state\n",
    "        self.bandits = np.array([\n",
    "            [0.2, 0.0, 0.1, -4.0],  # 4th arm best\n",
    "            [0.1, -5.0, 1.0, 0.25],  # 2nd arm best\n",
    "            [-3.5, 2.0, 3.2, 6.4]  # 1st arm best\n",
    "        ])\n",
    "        self.num_bandits, self.num_actions = self.bandits.shape\n",
    "\n",
    "    \n",
    "    def get_bandit(self):\n",
    "        self.active_bandit = np.random.randint(0, self.num_bandits)\n",
    "        return self.active_bandit\n",
    "\n",
    "    def pull(self, arm):\n",
    "        bandit = self.bandits[self.active_bandit, arm]\n",
    "        return 1 if np.random.randn(1) > bandit else -1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the agent.  \n",
    "Instead of doing the splice thing as before, we just do \n",
    "```python\n",
    "tf.matmul(context_one_hot, W)\n",
    "```\n",
    "which is followed up by the sigmoid and then argmax function.  \n",
    "OK, but why the extra sigmoid and argmax? Can't we just argmax without sigmoid?  \n",
    "Yeah, because it is sigmoid monotonic, you can. We just wanted it to look like an NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't simply run the code below. Take it from the github link of the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent1:\n",
    "\n",
    "    def __init__(self, learning_rate=1e-3, contexts=3, actions=4):\n",
    "        self.num_actions = actions\n",
    "        \n",
    "        self.reward_in = tf.placeholder(tf.float32, [1], name='reward_in')\n",
    "        self.context_in = tf.placeholder(tf.int32, [1], name='context_in')\n",
    "        self.action_in = tf.placeholder(tf.int32, [1], name='action_in')\n",
    "\n",
    "        # sess.run(best_action) to calculate the best action\n",
    "        context_one_hot = tf.one_hot(self.context_in, contexts)\n",
    "        W = tf.get_variable('W', [contexts, actions])\n",
    "        \n",
    "        self.output = tf.nn.sigmoid(tf.matmul(context_one_hot, W))\n",
    "        self.best_action = tf.argmax(self.output, axis=1)\n",
    "\n",
    "        # sess.run(optimizer) to update the best action\n",
    "        a_ = tf.reduce_sum(self.output * tf.one_hot(self.action_in, actions))\n",
    "        self.loss = -(tf.log(a_) * self.reward_in)\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.loss)\n",
    "\n",
    "    def predict(self, sess, context):\n",
    "        #The sess.run is returning a one element matrix, so to get the value from it\n",
    "        #we just do the [0] at the end\n",
    "        #ex. if a=[1] then a[0]=1\n",
    "        #we need the value not a matrix\n",
    "        return sess.run(self.best_action, {self.context_in: [context]})[0]\n",
    "\n",
    "    def random_or_predict(self, sess, epsilon, context):\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return self.predict(sess, context)\n",
    "\n",
    "    def train(self, sess, context, action, reward):\n",
    "        sess.run(self.optimizer, {\n",
    "            self.action_in: [action],\n",
    "            self.reward_in: [reward],\n",
    "            self.context_in: [context]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty straight forward.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env1 = ContextualBandit()\n",
    "agent1 = Agent1()\n",
    "num_episodes = 300000\n",
    "epsilon1 = 0.1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(num_episodes):\n",
    "        context = env1.get_bandit()\n",
    "        action = agent1.random_or_predict(sess, epsilon1, context)\n",
    "        reward = env1.pull(action)\n",
    "        # feed state, action, reward back to the policy network\n",
    "        agent1.train(sess, context, action, reward)\n",
    "        if ep % 500 == 0:\n",
    "            loss = sess.run(agent1.loss, {\n",
    "                agent1.action_in: [action],\n",
    "                agent1.reward_in: [reward],\n",
    "                agent1.context_in: [context]\n",
    "            })\n",
    "            print('Step {}, Loss={}'.format(ep, loss))\n",
    "    \n",
    "    # results time\n",
    "    print(np.argmin(env1.bandits, axis=1))\n",
    "    print('Best arm for Bandit 1:')\n",
    "    print(agent1.predict(sess, 0))\n",
    "\n",
    "    print('Best arm for Bandit 2:')\n",
    "    print(agent1.predict(sess, 1))\n",
    "\n",
    "    print('Best arm for Bandit 3:')\n",
    "    print(agent1.predict(sess, 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
