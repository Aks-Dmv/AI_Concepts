{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Architecture hand written code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "def init_weight(Mi, Mo):\n",
    "    return np.random.randn(Mi, Mo) / np.sqrt(Mi + Mo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will look at the whole class at once\n",
    "  \n",
    "#### We init the weights for I, O, f, Cbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, Mi, Mo, activation):\n",
    "        self.Mi = Mi\n",
    "        self.Mo = Mo\n",
    "        self.f  = activation\n",
    "\n",
    "        # numpy init\n",
    "        \n",
    "        Wxi = init_weight(Mi, Mo)\n",
    "        Whi = init_weight(Mo, Mo)\n",
    "        Wci = init_weight(Mo, Mo)\n",
    "        bi  = np.zeros(Mo)\n",
    "        Wxf = init_weight(Mi, Mo)\n",
    "        Whf = init_weight(Mo, Mo)\n",
    "        Wcf = init_weight(Mo, Mo)\n",
    "        bf  = np.zeros(Mo)\n",
    "        Wxc = init_weight(Mi, Mo)\n",
    "        Whc = init_weight(Mo, Mo)\n",
    "        bc  = np.zeros(Mo)\n",
    "        Wxo = init_weight(Mi, Mo)\n",
    "        Who = init_weight(Mo, Mo)\n",
    "        Wco = init_weight(Mo, Mo)\n",
    "        bo  = np.zeros(Mo)\n",
    "        c0  = np.zeros(Mo)\n",
    "        h0  = np.zeros(Mo)\n",
    "\n",
    "        # theano vars\n",
    "        self.Wxi = theano.shared(Wxi)\n",
    "        self.Whi = theano.shared(Whi)\n",
    "        self.Wci = theano.shared(Wci)\n",
    "        self.bi  = theano.shared(bi)\n",
    "        self.Wxf = theano.shared(Wxf)\n",
    "        self.Whf = theano.shared(Whf)\n",
    "        self.Wcf = theano.shared(Wcf)\n",
    "        self.bf  = theano.shared(bf)\n",
    "        self.Wxc = theano.shared(Wxc)\n",
    "        self.Whc = theano.shared(Whc)\n",
    "        self.bc  = theano.shared(bc)\n",
    "        self.Wxo = theano.shared(Wxo)\n",
    "        self.Who = theano.shared(Who)\n",
    "        self.Wco = theano.shared(Wco)\n",
    "        self.bo  = theano.shared(bo)\n",
    "        self.c0  = theano.shared(c0)\n",
    "        self.h0  = theano.shared(h0)\n",
    "        self.params = [\n",
    "            self.Wxi,\n",
    "            self.Whi,\n",
    "            self.Wci,\n",
    "            self.bi,\n",
    "            self.Wxf,\n",
    "            self.Whf,\n",
    "            self.Wcf,\n",
    "            self.bf,\n",
    "            self.Wxc,\n",
    "            self.Whc,\n",
    "            self.bc,\n",
    "            self.Wxo,\n",
    "            self.Who,\n",
    "            self.Wco,\n",
    "            self.bo,\n",
    "            self.c0,\n",
    "            self.h0,\n",
    "        ]\n",
    "\n",
    "    def recurrence(self, x_t, h_t1, c_t1):\n",
    "        i_t = T.nnet.sigmoid(x_t.dot(self.Wxi) + h_t1.dot(self.Whi) + c_t1.dot(self.Wci) + self.bi)\n",
    "        f_t = T.nnet.sigmoid(x_t.dot(self.Wxf) + h_t1.dot(self.Whf) + c_t1.dot(self.Wcf) + self.bf)\n",
    "        c_t = f_t * c_t1 + i_t * T.tanh(x_t.dot(self.Wxc) + h_t1.dot(self.Whc) + self.bc)\n",
    "        o_t = T.nnet.sigmoid(x_t.dot(self.Wxo) + h_t1.dot(self.Who) + c_t.dot(self.Wco) + self.bo)\n",
    "        h_t = o_t * T.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "    def output(self, x):\n",
    "        # input X should be a matrix (2-D)\n",
    "        # rows index time\n",
    "        [h, c], _ = theano.scan(\n",
    "            fn=self.recurrence,\n",
    "            sequences=x,\n",
    "            outputs_info=[self.h0, self.c0],\n",
    "            n_steps=x.shape[0],\n",
    "        )\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't use RELU in RNN because we need good initialization. We use tanh because the gradient doesn't vanish for quite some time, however the sigmoid gradient vanishes quite fast.  \n",
    "### ReLU isn't used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (batch)\n",
    "Almost the same, only we have different equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, Mi, Mo, activation):\n",
    "        self.Mi = Mi\n",
    "        self.Mo = Mo\n",
    "        self.f  = activation\n",
    "\n",
    "        # numpy init\n",
    "        Wxr = init_weight(Mi, Mo)\n",
    "        Whr = init_weight(Mo, Mo)\n",
    "        br  = np.zeros(Mo)\n",
    "        Wxz = init_weight(Mi, Mo)\n",
    "        Whz = init_weight(Mo, Mo)\n",
    "        bz  = np.zeros(Mo)\n",
    "        Wxh = init_weight(Mi, Mo)\n",
    "        Whh = init_weight(Mo, Mo)\n",
    "        bh  = np.zeros(Mo)\n",
    "        h0  = np.zeros(Mo)\n",
    "\n",
    "        # theano vars\n",
    "        self.Wxr = theano.shared(Wxr)\n",
    "        self.Whr = theano.shared(Whr)\n",
    "        self.br  = theano.shared(br)\n",
    "        self.Wxz = theano.shared(Wxz)\n",
    "        self.Whz = theano.shared(Whz)\n",
    "        self.bz  = theano.shared(bz)\n",
    "        self.Wxh = theano.shared(Wxh)\n",
    "        self.Whh = theano.shared(Whh)\n",
    "        self.bh  = theano.shared(bh)\n",
    "        self.h0  = theano.shared(h0)\n",
    "        self.params = [self.Wxr, self.Whr, self.br, self.Wxz, self.Whz, self.bz, self.Wxh, self.Whh, self.bh, self.h0]\n",
    "\n",
    "    def get_ht(self, x_t, h_t1):\n",
    "        r = T.nnet.sigmoid(x_t.dot(self.Wxr) + h_t1.dot(self.Whr) + self.br)\n",
    "        z = T.nnet.sigmoid(x_t.dot(self.Wxz) + h_t1.dot(self.Whz) + self.bz)\n",
    "        hhat = self.f(x_t.dot(self.Wxh) + (r * h_t1).dot(self.Whh) + self.bh)\n",
    "        h = (1 - z) * h_t1 + z * hhat\n",
    "        return h\n",
    "    \n",
    "    def recurrence(self, xWxr_t, xWxz_t, xWxh_t, is_start, h_t1, h0):\n",
    "        \n",
    "        # T.switch(condition, then_tensor, else_tensor)\n",
    "        # The switch function is just an if else function\n",
    "        \n",
    "        h_t = T.switch(\n",
    "          T.eq(is_start, 1),\n",
    "          self.get_ht(xWxr_t, xWxz_t, xWxh_t, h0),\n",
    "          self.get_ht(xWxr_t, xWxz_t, xWxh_t, h_t1)\n",
    "        )\n",
    "        return h_t\n",
    "\n",
    "    def output(self, Xflat, startPoints):\n",
    "        # Xflat should be (NT, D)\n",
    "        # calculate X after multiplying input weights\n",
    "        XWxr = Xflat.dot(self.Wxr)\n",
    "        XWxz = Xflat.dot(self.Wxz)\n",
    "        XWxh = Xflat.dot(self.Wxh)\n",
    "        # Scan is a way of applying a function on an input\n",
    "        h, _ = theano.scan(\n",
    "            fn=self.recurrence,\n",
    "            sequences=[XWxr, XWxz, XWxh, startPoints],\n",
    "            outputs_info=[self.h0],\n",
    "            non_sequences=[self.h0],\n",
    "            n_steps=Xflat.shape[0],\n",
    "        )\n",
    "        return h\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
